## llmock-o9r: Core handler with hardcoded echo response (OpenAI format)

Implemented the foundation of the mock LLM API server:

- Created `Server` struct with functional options pattern (`Option`, `New()`, `Handler()`)
- Implemented `POST /v1/chat/completions` endpoint that echoes the last user message
- Response follows OpenAI ChatCompletion format: id, object, created, model, choices, usage
- Token estimation based on word count (~1.3 tokens/word + per-message overhead)
- Proper error handling: 400 for bad JSON/empty messages, 405 for wrong method, 404 for unknown paths
- `cmd/llmock/main.go` starts server on configurable port (flag, env var, default 9090)
- 8 tests using httptest covering: echo response, usage stats, error cases, default model, fallback behavior
- All tests pass, build clean, vet clean

## llmock-bw3: Anthropic Messages API format

Added support for the Anthropic `/v1/messages` endpoint alongside the existing OpenAI endpoint:

- Introduced `Responder` interface (`Respond([]InternalMessage) (string, error)`) as the common generation abstraction
- Both API handlers convert their format-specific messages to `InternalMessage` and delegate to the same `Responder`
- `EchoResponder` implements the interface (extracted from inline logic in the OpenAI handler)
- `POST /v1/messages` accepts Anthropic Messages API requests (model, messages, max_tokens)
- Returns valid Anthropic response: `id` with `msg_` prefix + random hex, `type:"message"`, `role:"assistant"`, content array with `type:"text"` blocks, `stop_reason:"end_turn"`, usage with `input_tokens`/`output_tokens`
- 5 new tests: echo response with full structure validation, usage stats, empty messages error, invalid JSON error, cross-endpoint content consistency
- All 13 tests pass, build clean, vet clean

## llmock-64l: Regex rule matching with template expansion

Added the core rule-matching engine that replaces echo responses with configurable regex-based pattern matching:

- `Rule` type: compiled `*regexp.Regexp` pattern + slice of response template strings
- `RuleResponder` implements `Responder` interface; evaluates rules in order, first match wins
- Template expansion: `$1`, `$2`, etc. for capture groups; `${input}` for full original message
- If no rule matches, falls back to a hardcoded default response
- `WithRules(rules...)` functional option to configure the server
- `WithResponder(r)` generic functional option for any `Responder` implementation
- `NewRuleResponder(rules)` constructor; uses built-in default rules when passed empty slice
- Built-in default rules: greetings, "I need X", "how do I X", "what is X", "help me X", "can/could/would you X", thanks, farewell, and a catchall — all styled as a helpful AI assistant
- `LoadRulesFile(path)` and `ParseRulesYAML(data)` for loading rules from YAML config files
- Added `gopkg.in/yaml.v3` dependency for YAML parsing
- 14 new tests: match priority, capture group substitution, ${input} expansion, no-match fallback, random selection among multiple templates, default rules applied, Anthropic endpoint integration, WithResponder option, YAML parsing (valid, invalid regex, empty responses), file loading (valid, missing file), dollar-sign edge case
- All 27 tests pass, build clean, vet clean

## llmock-rgg: SSE streaming responses

Added Server-Sent Events streaming support for both OpenAI and Anthropic API formats:

- When `stream: true` is set in the request, responses are streamed token-by-token via SSE
- OpenAI streaming format: `data: {...}\n\n` chunks with `chat.completion.chunk` object type, `delta` containing content, final chunk with `finish_reason: "stop"`, terminated by `data: [DONE]\n\n`
- Anthropic streaming format: full event sequence — `message_start` → `content_block_start` → multiple `content_block_delta` events → `content_block_stop` → `message_delta` (with `stop_reason` and `output_tokens`) → `message_stop`
- Tokenization splits response text into randomized 1-3 word chunks for natural-feeling streaming
- Configurable inter-token delay via `llmock.WithTokenDelay(d)` (default 15ms)
- Proper SSE headers: `Content-Type: text/event-stream`, `Cache-Control: no-cache`, `Connection: keep-alive`
- Uses `http.Flusher` to flush after each chunk for real streaming behavior
- Handles client disconnection gracefully via context cancellation
- Non-streaming requests continue to work unchanged for both API formats
- New `stream.go` file with all streaming logic; handlers in `server.go` branch on `req.Stream`
- 9 new tests: OpenAI chunk format validation, content reconstruction, non-stream still works, Anthropic event sequence validation, Anthropic content reconstruction, non-stream still works, message_delta stop_reason, WithTokenDelay, consistent ID across chunks
- All 36 tests pass, build clean, vet clean

## llmock-0we: Runtime rule injection API

Added HTTP endpoints under `/_mock/` to inject, inspect, and reset rules at runtime for integration testing:

- `POST /_mock/rules` — inject one or more rules with JSON body; rules are prepended by default (priority 0), appended with priority -1, or inserted at a specific index
- `GET /_mock/rules` — return current rule list as JSON (patterns as strings, responses as arrays)
- `DELETE /_mock/rules` — reset rules to the initial startup configuration
- `POST /_mock/reset` — full reset: rules back to startup state and request log cleared
- `GET /_mock/requests` — return log of last 100 requests with timestamps, matched rule, user message, and response
- `DELETE /_mock/requests` — clear the request log
- New `adminState` type with `sync.RWMutex` for thread-safe mutable rule list and request log
- `adminResponder` wraps admin state with a fallback to the original responder (e.g., EchoResponder) when no rule matches — injected rules work even on servers started without rules
- `WithAdminAPI(false)` option to disable the admin endpoints for production-like usage
- Admin API is enabled by default on all servers
- Refactored `extractInput()` helper and `errNoMessages` sentinel error as shared utilities across EchoResponder, RuleResponder, and adminResponder
- Request logging captures method, path, user message, matched rule pattern, and response text for each API request
- 16 new tests: rule injection with match verification, prepend/append priority, GET/DELETE rules, full reset, request log inspection and clearing, admin disabled returns 404, invalid regex/empty rules/empty responses validation, 100-entry log cap, default server admin endpoints, rule injection on echo-only server, concurrent access with race detector
- All 52 tests pass, build clean, vet clean, no race conditions

## llmock-6dk: Failure and delay injection

Added fault injection system for simulating failures, errors, and latency — critical for testing retry logic, timeouts, and error handling:

- Five fault types: `error` (HTTP error responses), `delay` (adds latency before normal response), `timeout` (accepts connection then hangs until client disconnects), `malformed` (returns invalid JSON or broken SSE), `rate_limit` (429 with Retry-After header)
- `Fault` struct with configurable fields: `Type`, `Status`, `Message`, `ErrorType`, `DelayMS`, `Probability` (0-1 for random firing), `Count` (auto-clearing after N triggers)
- Correct error response formats for both APIs: OpenAI `{"error":{"message":"...","type":"..."}}` and Anthropic `{"type":"error","error":{"type":"...","message":"..."}}`
- Faults evaluated before rules in the request pipeline — global faults fire first
- Go API: `llmock.WithFault(llmock.Fault{...})` option for startup configuration
- `llmock.WithSeed(int64)` option for deterministic fault probability in tests
- Admin API endpoints for runtime fault management:
  - `POST /_mock/faults` — inject faults with JSON body `{"faults":[...]}`
  - `GET /_mock/faults` — inspect current active faults
  - `DELETE /_mock/faults` — clear all faults
- Thread-safe `faultState` with mutex-protected fault list and dedicated `math/rand/v2.Rand` instance
- Count-based faults auto-remove from the active list when exhausted
- Timeout fault starts SSE stream (for streaming requests) then blocks, simulating mid-response timeout
- Malformed fault returns broken JSON for non-streaming and broken SSE for streaming requests
- 16 new tests: error faults on both OpenAI and Anthropic endpoints, rate limit with Retry-After header on both APIs, delay with timing verification, malformed for both stream and non-stream, timeout with context deadline, count-based auto-clearing, probability with fixed seed, admin API POST/GET/DELETE, fault-before-rules ordering, Go API WithFault, default error status
- All 68 tests pass, build clean, vet clean, no race conditions

## llmock-w2p: Markov chain text generator

Added a Markov chain text generator that produces LLM-ish filler text, used as a fallback and for padding template responses:

- `MarkovChain` type with configurable chain order (default 2 — bigram prefix), trained on whitespace-delimited tokens
- Generation stops at a configurable max token count or when it hits a natural sentence ending (`.`, `!`, `?`)
- Deterministic output with sorted key selection for reproducible generation given a fixed seed
- Default corpus embedded via `//go:embed` containing ~2000 words of "helpful AI assistant" style text — stereotypical LLM responses about architecture, testing, best practices, security, etc.
- `llmock.WithCorpus(r io.Reader)` option to provide a custom training corpus
- `llmock.WithCorpusFile(path string)` convenience option to load corpus from a file
- `llmock.WithSeed(int64)` now also seeds the Markov generator for deterministic test output
- `MarkovResponder` implements the `Responder` interface and serves as the fallback for `RuleResponder` when no rules match (replaces the old hardcoded fallback string)
- Template responses support `{{markov}}` (default 100 token limit) and `{{markov:N}}` (custom token limit) placeholders to splice Markov-generated text into rule responses
- `MarkovChain` is built once at startup and is safe for concurrent reads; `MarkovResponder` uses a mutex to protect its RNG
- `DefaultCorpusText()` exported function for test access to the embedded corpus
- `NewMarkovResponder(seed)` and `NewMarkovChain(order)` constructors for programmatic use
- 15 new tests: deterministic output with fixed seed, different seeds produce different output, output contains only corpus words (statistical), default corpus words-only validation, max token limit respected, empty corpus handling, order-1 chain, server integration (rule match vs Markov fallback), deterministic MarkovResponder, no-messages error, `{{markov}}` template expansion, `{{markov:N}}` with token limit, WithCorpus custom corpus, WithCorpusFile, Anthropic endpoint integration
- All 83 tests pass, build clean, vet clean, no race conditions

## llmock-5bz: YAML/JSON config file and CLI polish

Added config file support and made the standalone CLI server production-ready:

- `Config` struct unifies YAML/JSON file config and functional options into one source of truth
  - `ServerConfig`: port, admin_api toggle
  - `DefaultConfig`: token_delay_ms, seed, model
  - `RuleConfig`: pattern, responses, delay_ms
  - Supports `corpus_file` and `faults` array at the top level
- `LoadConfig(path)` reads a YAML or JSON config file (auto-detected by `.json` extension)
- `ParseConfig(data, path)` parses config from raw bytes
- `FindDefaultConfig()` auto-discovers `llmock.yaml` or `llmock.json` in the current directory
- `CompileRules(configs)` converts `RuleConfig` entries to compiled `Rule` values with validation
- `Config.ToOptions()` converts the full config into functional `Option` values for `New()`, so config files and programmatic options use the same code path; functional options applied after config options override config values
- CLI flags: `--config` (explicit config path), `--port` (overrides config), `--verbose` (log all requests to stderr)
- Port resolution priority: `--port` flag > config file > `PORT` env var > 9090
- Verbose mode middleware logs: method, path, HTTP status, and response time for each request
- `responseWriter` wrapper captures status codes and delegates `Flush()` for SSE compatibility
- Startup banner: logs config source, port, number of rules loaded, corpus source, admin API status
- Graceful shutdown on SIGINT/SIGTERM with 5-second timeout via `http.Server.Shutdown()`
- 15 new tests: YAML parsing (full config), JSON parsing, invalid YAML/JSON errors, file loading (valid and missing), `FindDefaultConfig` (none, yaml, json), `CompileRules` (valid, invalid regex, no responses), `ToOptions` (full config, invalid rule, empty config), config with faults, token delay option
- All 98 tests pass, build clean, vet clean, no race conditions

## llmock-ass: Tool call rule matching and OpenAI/Anthropic response format

Added tool use / function calling simulation — rules can now respond with tool calls instead of text, enabling testing of tool-use workflows:

- New `ToolCallConfig` struct: `name` (tool function name) + `arguments` (map with template expansion support for `$1`, `$2`, `${input}`)
- `Rule` extended with optional `ToolCall *ToolCallConfig` field — rules can specify either text responses or a tool call
- `Response` type replaces raw `string` as the return value from `Responder.Respond()`, carrying either `Text` or `ToolCalls`
- `Responder` interface updated: `Respond([]InternalMessage) (Response, error)` — all implementations updated (EchoResponder, RuleResponder, MarkovResponder, adminResponder)
- OpenAI tool call response format: `choices[0].message.tool_calls` array with `id` (prefix `call_`), `type:"function"`, `function.name`, `function.arguments` (JSON string)
- Anthropic tool call response format: `content` array with `type:"tool_use"` blocks containing `id` (prefix `toolu_`), `name`, `input` (object)
- Tool validation: when request includes `tools` array, only tool calls matching defined tool names are returned; non-matching tool calls fall through to text response
- OpenAI request type extended with `Tools []OpenAIToolDef` (type + function name/description/parameters)
- Anthropic request type extended with `Tools []AnthropicToolDef` (name, description, input_schema)
- `ChoiceMessage` type replaces `Message` in response choices to support both `content` (text) and `tool_calls` fields with proper JSON omitempty
- Streaming tool calls for OpenAI: chunks stream function name first, then arguments in 20-byte increments, final chunk with `finish_reason:"tool_calls"`
- Streaming tool calls for Anthropic: `content_block_start` with `tool_use` type, `content_block_delta` with `input_json_delta` partial chunks, `message_delta` with `stop_reason:"tool_use"`
- Config file support: `tool_call` field in YAML/JSON rule config with `name` and `arguments`
- `CompileRules` and `ParseRulesYAML` updated to accept rules with `tool_call` instead of `responses`
- Tool call IDs use `crypto/rand` for realistic-looking hex suffixes
- 10 new tests: OpenAI basic tool call, Anthropic basic tool call, fall-through when tool not in request, capture group expansion in arguments, config file parsing, OpenAI streaming tool call, Anthropic streaming tool call, no-tools-in-request behavior, text response unchanged, ${input} template in arguments
- All 108 tests pass, build clean, vet clean

## llmock-bdd: Auto-generation mode for tool calls from JSON schema

Added auto-generation of tool calls from request tool definitions — when enabled, the server generates realistic tool calls from JSON schema without needing explicit rules:

- New `WithAutoToolCalls(bool)` option: enables auto-generation mode (disabled by default)
- New `autotool.go`: `generateToolCallFromSchema` picks a random tool from the request and generates arguments conforming to its JSON schema
- JSON schema support: handles `object`, `array`, `string`, `number`, `integer`, `boolean`, `null` types
- `enum` support: picks a random value from the enum list
- `required` properties: always included; optional properties included 50% of the time
- Nested schemas: objects within objects, arrays with typed items
- Format-aware string generation: `date` (YYYY-MM-DD), `date-time` (ISO 8601), `email`, `uri`/`url`
- Description-aware string generation: generates city names for location/city fields, names for name fields, language codes for language fields
- Array generation: 1-3 items matching the `items` schema
- Integration in both handlers: auto-generation triggers when `autoToolCalls` is true, no rule produced a tool call, and tools are defined in the request
- Helper functions: `openAIToRequestTools` and `anthropicToRequestTools` convert API-specific tool defs to internal `RequestTool` format
- Config file support: `auto_tool_calls` boolean in `defaults` section of YAML/JSON config
- Works with streaming: auto-generated tool calls stream identically to rule-based tool calls
- Rule precedence preserved: explicit tool call rules always take priority over auto-generation
- 10 new tests: OpenAI auto-generation, Anthropic auto-generation, disabled mode returns text, rule match takes precedence, multiple tools picks one, complex nested schema, no tools means no auto-gen, streaming with auto-gen, config parsing, empty parameters
- All 118 tests pass, build clean, vet clean, no race conditions

## llmock-2fo: MCP core types and JSON-RPC 2.0 message handling

Added MCP (Model Context Protocol) server simulation with full JSON-RPC 2.0 support, enabling testing of MCP client integrations without a real MCP server:

- New `mcp.go` file with complete MCP implementation:
  - JSON-RPC 2.0 types: `jsonRPCRequest`, `jsonRPCResponse`, `jsonRPCErr` with standard error codes (parse error -32700, invalid request -32600, method not found -32601, invalid params -32602)
  - MCP config types: `MCPToolConfig` (name, description, input_schema, pattern-matched responses), `MCPResourceConfig` (uri, name, mime_type, content), `MCPPromptConfig` (name, description, arguments, template)
  - `MCPConfig` struct aggregating tools, resources, and prompts
  - Thread-safe `mcpState` with `sync.RWMutex` for concurrent access to tools, resources, and prompts
- `POST /mcp` endpoint handling all core MCP methods:
  - `initialize` — returns protocol version, server info (name: "llmock"), and capabilities (tools, resources, prompts)
  - `tools/list` — returns configured tools with name, description, and inputSchema
  - `tools/call` — executes tool call with pattern matching on JSON-serialized arguments; falls back to Markov-generated text when no pattern matches
  - `resources/list` — returns configured resources with uri, name, and optional mimeType
  - `resources/read` — returns resource content by URI with mimeType (defaults to "text/plain")
  - `prompts/list` — returns configured prompts with name, description, and arguments
  - `prompts/get` — returns prompt with `{{argName}}` template expansion from provided arguments
- MCP is optional — disabled by default, enabled via `WithMCP(MCPConfig{...})` option or `mcp:` section in YAML/JSON config
- Runtime injection via admin API endpoints:
  - `GET/POST/DELETE /_mock/mcp/tools` — inspect, add, and clear MCP tools
  - `GET/POST/DELETE /_mock/mcp/resources` — inspect, add, and clear MCP resources
  - `GET/POST/DELETE /_mock/mcp/prompts` — inspect, add, and clear MCP prompts
- Config file integration: `mcp` section in YAML/JSON config with `tools`, `resources`, `prompts` arrays
- Tool call pattern matching: responses support regex patterns matched against JSON-serialized arguments (same approach as main API rule matching)
- 26 new tests: disabled-by-default, initialize handshake, invalid JSON (parse error), invalid JSON-RPC version, method not found, tools/list (with tools and empty), tools/call (pattern matching, second pattern, unknown tool, missing name, Markov fallback), resources/list (with mimeType), resources/read (found, not found, missing uri), prompts/list, prompts/get (template expansion, not found, missing name), request ID preservation (integer and string), full handshake sequence (7-step), admin tools/resources/prompts endpoints (GET/POST/DELETE), config YAML integration
- All 144 tests pass, build clean, vet clean
