## llmock-o9r: Core handler with hardcoded echo response (OpenAI format)

Implemented the foundation of the mock LLM API server:

- Created `Server` struct with functional options pattern (`Option`, `New()`, `Handler()`)
- Implemented `POST /v1/chat/completions` endpoint that echoes the last user message
- Response follows OpenAI ChatCompletion format: id, object, created, model, choices, usage
- Token estimation based on word count (~1.3 tokens/word + per-message overhead)
- Proper error handling: 400 for bad JSON/empty messages, 405 for wrong method, 404 for unknown paths
- `cmd/llmock/main.go` starts server on configurable port (flag, env var, default 9090)
- 8 tests using httptest covering: echo response, usage stats, error cases, default model, fallback behavior
- All tests pass, build clean, vet clean

## llmock-bw3: Anthropic Messages API format

Added support for the Anthropic `/v1/messages` endpoint alongside the existing OpenAI endpoint:

- Introduced `Responder` interface (`Respond([]InternalMessage) (string, error)`) as the common generation abstraction
- Both API handlers convert their format-specific messages to `InternalMessage` and delegate to the same `Responder`
- `EchoResponder` implements the interface (extracted from inline logic in the OpenAI handler)
- `POST /v1/messages` accepts Anthropic Messages API requests (model, messages, max_tokens)
- Returns valid Anthropic response: `id` with `msg_` prefix + random hex, `type:"message"`, `role:"assistant"`, content array with `type:"text"` blocks, `stop_reason:"end_turn"`, usage with `input_tokens`/`output_tokens`
- 5 new tests: echo response with full structure validation, usage stats, empty messages error, invalid JSON error, cross-endpoint content consistency
- All 13 tests pass, build clean, vet clean

## llmock-64l: Regex rule matching with template expansion

Added the core rule-matching engine that replaces echo responses with configurable regex-based pattern matching:

- `Rule` type: compiled `*regexp.Regexp` pattern + slice of response template strings
- `RuleResponder` implements `Responder` interface; evaluates rules in order, first match wins
- Template expansion: `$1`, `$2`, etc. for capture groups; `${input}` for full original message
- If no rule matches, falls back to a hardcoded default response
- `WithRules(rules...)` functional option to configure the server
- `WithResponder(r)` generic functional option for any `Responder` implementation
- `NewRuleResponder(rules)` constructor; uses built-in default rules when passed empty slice
- Built-in default rules: greetings, "I need X", "how do I X", "what is X", "help me X", "can/could/would you X", thanks, farewell, and a catchall — all styled as a helpful AI assistant
- `LoadRulesFile(path)` and `ParseRulesYAML(data)` for loading rules from YAML config files
- Added `gopkg.in/yaml.v3` dependency for YAML parsing
- 14 new tests: match priority, capture group substitution, ${input} expansion, no-match fallback, random selection among multiple templates, default rules applied, Anthropic endpoint integration, WithResponder option, YAML parsing (valid, invalid regex, empty responses), file loading (valid, missing file), dollar-sign edge case
- All 27 tests pass, build clean, vet clean

## llmock-rgg: SSE streaming responses

Added Server-Sent Events streaming support for both OpenAI and Anthropic API formats:

- When `stream: true` is set in the request, responses are streamed token-by-token via SSE
- OpenAI streaming format: `data: {...}\n\n` chunks with `chat.completion.chunk` object type, `delta` containing content, final chunk with `finish_reason: "stop"`, terminated by `data: [DONE]\n\n`
- Anthropic streaming format: full event sequence — `message_start` → `content_block_start` → multiple `content_block_delta` events → `content_block_stop` → `message_delta` (with `stop_reason` and `output_tokens`) → `message_stop`
- Tokenization splits response text into randomized 1-3 word chunks for natural-feeling streaming
- Configurable inter-token delay via `llmock.WithTokenDelay(d)` (default 15ms)
- Proper SSE headers: `Content-Type: text/event-stream`, `Cache-Control: no-cache`, `Connection: keep-alive`
- Uses `http.Flusher` to flush after each chunk for real streaming behavior
- Handles client disconnection gracefully via context cancellation
- Non-streaming requests continue to work unchanged for both API formats
- New `stream.go` file with all streaming logic; handlers in `server.go` branch on `req.Stream`
- 9 new tests: OpenAI chunk format validation, content reconstruction, non-stream still works, Anthropic event sequence validation, Anthropic content reconstruction, non-stream still works, message_delta stop_reason, WithTokenDelay, consistent ID across chunks
- All 36 tests pass, build clean, vet clean

## llmock-0we: Runtime rule injection API

Added HTTP endpoints under `/_mock/` to inject, inspect, and reset rules at runtime for integration testing:

- `POST /_mock/rules` — inject one or more rules with JSON body; rules are prepended by default (priority 0), appended with priority -1, or inserted at a specific index
- `GET /_mock/rules` — return current rule list as JSON (patterns as strings, responses as arrays)
- `DELETE /_mock/rules` — reset rules to the initial startup configuration
- `POST /_mock/reset` — full reset: rules back to startup state and request log cleared
- `GET /_mock/requests` — return log of last 100 requests with timestamps, matched rule, user message, and response
- `DELETE /_mock/requests` — clear the request log
- New `adminState` type with `sync.RWMutex` for thread-safe mutable rule list and request log
- `adminResponder` wraps admin state with a fallback to the original responder (e.g., EchoResponder) when no rule matches — injected rules work even on servers started without rules
- `WithAdminAPI(false)` option to disable the admin endpoints for production-like usage
- Admin API is enabled by default on all servers
- Refactored `extractInput()` helper and `errNoMessages` sentinel error as shared utilities across EchoResponder, RuleResponder, and adminResponder
- Request logging captures method, path, user message, matched rule pattern, and response text for each API request
- 16 new tests: rule injection with match verification, prepend/append priority, GET/DELETE rules, full reset, request log inspection and clearing, admin disabled returns 404, invalid regex/empty rules/empty responses validation, 100-entry log cap, default server admin endpoints, rule injection on echo-only server, concurrent access with race detector
- All 52 tests pass, build clean, vet clean, no race conditions

## llmock-6dk: Failure and delay injection

Added fault injection system for simulating failures, errors, and latency — critical for testing retry logic, timeouts, and error handling:

- Five fault types: `error` (HTTP error responses), `delay` (adds latency before normal response), `timeout` (accepts connection then hangs until client disconnects), `malformed` (returns invalid JSON or broken SSE), `rate_limit` (429 with Retry-After header)
- `Fault` struct with configurable fields: `Type`, `Status`, `Message`, `ErrorType`, `DelayMS`, `Probability` (0-1 for random firing), `Count` (auto-clearing after N triggers)
- Correct error response formats for both APIs: OpenAI `{"error":{"message":"...","type":"..."}}` and Anthropic `{"type":"error","error":{"type":"...","message":"..."}}`
- Faults evaluated before rules in the request pipeline — global faults fire first
- Go API: `llmock.WithFault(llmock.Fault{...})` option for startup configuration
- `llmock.WithSeed(int64)` option for deterministic fault probability in tests
- Admin API endpoints for runtime fault management:
  - `POST /_mock/faults` — inject faults with JSON body `{"faults":[...]}`
  - `GET /_mock/faults` — inspect current active faults
  - `DELETE /_mock/faults` — clear all faults
- Thread-safe `faultState` with mutex-protected fault list and dedicated `math/rand/v2.Rand` instance
- Count-based faults auto-remove from the active list when exhausted
- Timeout fault starts SSE stream (for streaming requests) then blocks, simulating mid-response timeout
- Malformed fault returns broken JSON for non-streaming and broken SSE for streaming requests
- 16 new tests: error faults on both OpenAI and Anthropic endpoints, rate limit with Retry-After header on both APIs, delay with timing verification, malformed for both stream and non-stream, timeout with context deadline, count-based auto-clearing, probability with fixed seed, admin API POST/GET/DELETE, fault-before-rules ordering, Go API WithFault, default error status
- All 68 tests pass, build clean, vet clean, no race conditions

## llmock-w2p: Markov chain text generator

Added a Markov chain text generator that produces LLM-ish filler text, used as a fallback and for padding template responses:

- `MarkovChain` type with configurable chain order (default 2 — bigram prefix), trained on whitespace-delimited tokens
- Generation stops at a configurable max token count or when it hits a natural sentence ending (`.`, `!`, `?`)
- Deterministic output with sorted key selection for reproducible generation given a fixed seed
- Default corpus embedded via `//go:embed` containing ~2000 words of "helpful AI assistant" style text — stereotypical LLM responses about architecture, testing, best practices, security, etc.
- `llmock.WithCorpus(r io.Reader)` option to provide a custom training corpus
- `llmock.WithCorpusFile(path string)` convenience option to load corpus from a file
- `llmock.WithSeed(int64)` now also seeds the Markov generator for deterministic test output
- `MarkovResponder` implements the `Responder` interface and serves as the fallback for `RuleResponder` when no rules match (replaces the old hardcoded fallback string)
- Template responses support `{{markov}}` (default 100 token limit) and `{{markov:N}}` (custom token limit) placeholders to splice Markov-generated text into rule responses
- `MarkovChain` is built once at startup and is safe for concurrent reads; `MarkovResponder` uses a mutex to protect its RNG
- `DefaultCorpusText()` exported function for test access to the embedded corpus
- `NewMarkovResponder(seed)` and `NewMarkovChain(order)` constructors for programmatic use
- 15 new tests: deterministic output with fixed seed, different seeds produce different output, output contains only corpus words (statistical), default corpus words-only validation, max token limit respected, empty corpus handling, order-1 chain, server integration (rule match vs Markov fallback), deterministic MarkovResponder, no-messages error, `{{markov}}` template expansion, `{{markov:N}}` with token limit, WithCorpus custom corpus, WithCorpusFile, Anthropic endpoint integration
- All 83 tests pass, build clean, vet clean, no race conditions

## llmock-5bz: YAML/JSON config file and CLI polish

Added config file support and made the standalone CLI server production-ready:

- `Config` struct unifies YAML/JSON file config and functional options into one source of truth
  - `ServerConfig`: port, admin_api toggle
  - `DefaultConfig`: token_delay_ms, seed, model
  - `RuleConfig`: pattern, responses, delay_ms
  - Supports `corpus_file` and `faults` array at the top level
- `LoadConfig(path)` reads a YAML or JSON config file (auto-detected by `.json` extension)
- `ParseConfig(data, path)` parses config from raw bytes
- `FindDefaultConfig()` auto-discovers `llmock.yaml` or `llmock.json` in the current directory
- `CompileRules(configs)` converts `RuleConfig` entries to compiled `Rule` values with validation
- `Config.ToOptions()` converts the full config into functional `Option` values for `New()`, so config files and programmatic options use the same code path; functional options applied after config options override config values
- CLI flags: `--config` (explicit config path), `--port` (overrides config), `--verbose` (log all requests to stderr)
- Port resolution priority: `--port` flag > config file > `PORT` env var > 9090
- Verbose mode middleware logs: method, path, HTTP status, and response time for each request
- `responseWriter` wrapper captures status codes and delegates `Flush()` for SSE compatibility
- Startup banner: logs config source, port, number of rules loaded, corpus source, admin API status
- Graceful shutdown on SIGINT/SIGTERM with 5-second timeout via `http.Server.Shutdown()`
- 15 new tests: YAML parsing (full config), JSON parsing, invalid YAML/JSON errors, file loading (valid and missing), `FindDefaultConfig` (none, yaml, json), `CompileRules` (valid, invalid regex, no responses), `ToOptions` (full config, invalid rule, empty config), config with faults, token delay option
- All 98 tests pass, build clean, vet clean, no race conditions

## llmock-ass: Tool call rule matching and OpenAI/Anthropic response format

Added tool use / function calling simulation — rules can now respond with tool calls instead of text, enabling testing of tool-use workflows:

- New `ToolCallConfig` struct: `name` (tool function name) + `arguments` (map with template expansion support for `$1`, `$2`, `${input}`)
- `Rule` extended with optional `ToolCall *ToolCallConfig` field — rules can specify either text responses or a tool call
- `Response` type replaces raw `string` as the return value from `Responder.Respond()`, carrying either `Text` or `ToolCalls`
- `Responder` interface updated: `Respond([]InternalMessage) (Response, error)` — all implementations updated (EchoResponder, RuleResponder, MarkovResponder, adminResponder)
- OpenAI tool call response format: `choices[0].message.tool_calls` array with `id` (prefix `call_`), `type:"function"`, `function.name`, `function.arguments` (JSON string)
- Anthropic tool call response format: `content` array with `type:"tool_use"` blocks containing `id` (prefix `toolu_`), `name`, `input` (object)
- Tool validation: when request includes `tools` array, only tool calls matching defined tool names are returned; non-matching tool calls fall through to text response
- OpenAI request type extended with `Tools []OpenAIToolDef` (type + function name/description/parameters)
- Anthropic request type extended with `Tools []AnthropicToolDef` (name, description, input_schema)
- `ChoiceMessage` type replaces `Message` in response choices to support both `content` (text) and `tool_calls` fields with proper JSON omitempty
- Streaming tool calls for OpenAI: chunks stream function name first, then arguments in 20-byte increments, final chunk with `finish_reason:"tool_calls"`
- Streaming tool calls for Anthropic: `content_block_start` with `tool_use` type, `content_block_delta` with `input_json_delta` partial chunks, `message_delta` with `stop_reason:"tool_use"`
- Config file support: `tool_call` field in YAML/JSON rule config with `name` and `arguments`
- `CompileRules` and `ParseRulesYAML` updated to accept rules with `tool_call` instead of `responses`
- Tool call IDs use `crypto/rand` for realistic-looking hex suffixes
- 10 new tests: OpenAI basic tool call, Anthropic basic tool call, fall-through when tool not in request, capture group expansion in arguments, config file parsing, OpenAI streaming tool call, Anthropic streaming tool call, no-tools-in-request behavior, text response unchanged, ${input} template in arguments
- All 108 tests pass, build clean, vet clean

## llmock-bdd: Auto-generation mode for tool calls from JSON schema

Added auto-generation of tool calls from request tool definitions — when enabled, the server generates realistic tool calls from JSON schema without needing explicit rules:

- New `WithAutoToolCalls(bool)` option: enables auto-generation mode (disabled by default)
- New `autotool.go`: `generateToolCallFromSchema` picks a random tool from the request and generates arguments conforming to its JSON schema
- JSON schema support: handles `object`, `array`, `string`, `number`, `integer`, `boolean`, `null` types
- `enum` support: picks a random value from the enum list
- `required` properties: always included; optional properties included 50% of the time
- Nested schemas: objects within objects, arrays with typed items
- Format-aware string generation: `date` (YYYY-MM-DD), `date-time` (ISO 8601), `email`, `uri`/`url`
- Description-aware string generation: generates city names for location/city fields, names for name fields, language codes for language fields
- Array generation: 1-3 items matching the `items` schema
- Integration in both handlers: auto-generation triggers when `autoToolCalls` is true, no rule produced a tool call, and tools are defined in the request
- Helper functions: `openAIToRequestTools` and `anthropicToRequestTools` convert API-specific tool defs to internal `RequestTool` format
- Config file support: `auto_tool_calls` boolean in `defaults` section of YAML/JSON config
- Works with streaming: auto-generated tool calls stream identically to rule-based tool calls
- Rule precedence preserved: explicit tool call rules always take priority over auto-generation
- 10 new tests: OpenAI auto-generation, Anthropic auto-generation, disabled mode returns text, rule match takes precedence, multiple tools picks one, complex nested schema, no tools means no auto-gen, streaming with auto-gen, config parsing, empty parameters
- All 118 tests pass, build clean, vet clean, no race conditions

## llmock-2fo: MCP core types and JSON-RPC 2.0 message handling

Added MCP (Model Context Protocol) server simulation with full JSON-RPC 2.0 support, enabling testing of MCP client integrations without a real MCP server:

- New `mcp.go` file with complete MCP implementation:
  - JSON-RPC 2.0 types: `jsonRPCRequest`, `jsonRPCResponse`, `jsonRPCErr` with standard error codes (parse error -32700, invalid request -32600, method not found -32601, invalid params -32602)
  - MCP config types: `MCPToolConfig` (name, description, input_schema, pattern-matched responses), `MCPResourceConfig` (uri, name, mime_type, content), `MCPPromptConfig` (name, description, arguments, template)
  - `MCPConfig` struct aggregating tools, resources, and prompts
  - Thread-safe `mcpState` with `sync.RWMutex` for concurrent access to tools, resources, and prompts
- `POST /mcp` endpoint handling all core MCP methods:
  - `initialize` — returns protocol version, server info (name: "llmock"), and capabilities (tools, resources, prompts)
  - `tools/list` — returns configured tools with name, description, and inputSchema
  - `tools/call` — executes tool call with pattern matching on JSON-serialized arguments; falls back to Markov-generated text when no pattern matches
  - `resources/list` — returns configured resources with uri, name, and optional mimeType
  - `resources/read` — returns resource content by URI with mimeType (defaults to "text/plain")
  - `prompts/list` — returns configured prompts with name, description, and arguments
  - `prompts/get` — returns prompt with `{{argName}}` template expansion from provided arguments
- MCP is optional — disabled by default, enabled via `WithMCP(MCPConfig{...})` option or `mcp:` section in YAML/JSON config
- Runtime injection via admin API endpoints:
  - `GET/POST/DELETE /_mock/mcp/tools` — inspect, add, and clear MCP tools
  - `GET/POST/DELETE /_mock/mcp/resources` — inspect, add, and clear MCP resources
  - `GET/POST/DELETE /_mock/mcp/prompts` — inspect, add, and clear MCP prompts
- Config file integration: `mcp` section in YAML/JSON config with `tools`, `resources`, `prompts` arrays
- Tool call pattern matching: responses support regex patterns matched against JSON-serialized arguments (same approach as main API rule matching)
- 26 new tests: disabled-by-default, initialize handshake, invalid JSON (parse error), invalid JSON-RPC version, method not found, tools/list (with tools and empty), tools/call (pattern matching, second pattern, unknown tool, missing name, Markov fallback), resources/list (with mimeType), resources/read (found, not found, missing uri), prompts/list, prompts/get (template expansion, not found, missing name), request ID preservation (integer and string), full handshake sequence (7-step), admin tools/resources/prompts endpoints (GET/POST/DELETE), config YAML integration
- All 144 tests pass, build clean, vet clean

## llmock-09x: MCP: Config loading for tools, resources, and prompts

Verified that MCP config loading for tools, resources, and prompts was already fully implemented as part of the llmock-2fo dependency:

- `Config.MCP *MCPConfig` field in config.go:24 with proper YAML/JSON struct tags
- `MCPToolConfig`, `MCPResourceConfig`, `MCPPromptConfig` types in mcp.go:46-87 all have `yaml`/`json` tags for parsing
- `Config.ToOptions()` in config.go:144-146 converts parsed MCP config to `WithMCP()` functional option
- `TestMCPConfigIntegration` in mcp_test.go:865-923 validates the full flow: YAML config with tools/resources/prompts -> `ParseConfig()` -> `ToOptions()` -> `New()` -> working MCP server endpoint
- All 144 tests pass, build clean, vet clean

## llmock-ccu: MCP: POST /mcp endpoint with initialize, tools/list, tools/call

Verified that this task was already fully implemented as part of llmock-2fo. The POST /mcp endpoint with all required methods (initialize, tools/list, tools/call) is working and tested:

- `POST /mcp` route registered in server.go:129 when MCP is enabled
- `handleMCP` handler in mcp.go parses JSON-RPC 2.0 requests and dispatches to method handlers
- `initialize` returns protocol version "2025-03-26", server info, and capabilities
- `tools/list` returns configured tools with name, description, and inputSchema
- `tools/call` executes tool calls with regex pattern matching on arguments; Markov fallback when no pattern matches
- All 26 MCP tests pass (144 total), build clean, vet clean
- Closed as already complete — no additional code changes needed

## llmock-c5y: Multi-turn tool use conversation support

Added support for multi-turn tool use conversations in both OpenAI and Anthropic API formats, enabling realistic agentic workflow testing:

- **OpenAI `Message` type** updated to support the full tool use protocol:
  - `Content` field changed from `string` to `json.RawMessage` to handle both string and `null` values (assistant messages with tool calls have null content)
  - Added `ToolCalls []OpenAIToolCall` field for assistant messages that invoke tools
  - Added `ToolCallID string` field for tool-role result messages linking back to a tool call
  - Added `Name string` field for function name in tool messages
  - `MessageContent()` method safely extracts text from the raw JSON content
- **Anthropic `AnthropicMessage` type** updated to support content block arrays:
  - `Content` field changed from `string` to `json.RawMessage` to handle both string and array-of-blocks formats
  - New `AnthropicInputBlock` type representing request-side content blocks: `text`, `tool_use` (with id/name/input), and `tool_result` (with tool_use_id/content/is_error)
  - `MessageContent()` method handles string content, array content blocks, and nested tool_result content (both string and array-of-blocks formats)
- **Internal conversion** updated for both APIs:
  - `toInternalMessages` skips assistant messages with tool_calls but no text content (they don't contain text for rule matching)
  - `anthropicToInternal` skips assistant messages with only tool_use blocks
  - Both preserve tool-role and tool_result messages for `extractInput` to consider
- **Token estimation** updated to use `MessageContent()` for both APIs
- 10 new tests: OpenAI multi-turn with tool results, OpenAI tool result rule matching, OpenAI null content handling, Anthropic tool_use/tool_result conversation, Anthropic nested content blocks in tool_result, Anthropic mixed text + tool_result blocks, OpenAI multiple parallel tool calls, OpenAI streaming with tool results, Anthropic streaming with tool results, Anthropic tool_result with is_error flag
- All 154 tests pass, build clean, vet clean, no race conditions

## llmock-xva: Make it work out of the box

Changed the default zero-config behavior so that `llmock.New()` and `go run ./cmd/llmock` produce useful AI-assistant-like responses without any configuration:

- Default responder changed from `EchoResponder` to `RuleResponder` with built-in default rules
- `NewRuleResponder(nil)` is now used when no responder is explicitly configured, which triggers 8 built-in regex patterns (greetings, "how do I X", "what is X", "help me X", "can you X", thanks, farewell, catch-all) with Markov chain fallback
- Previously, zero-config mode simply echoed back the user's input verbatim — now it produces varied, context-aware responses like a real LLM mock should
- `EchoResponder` remains available via `WithResponder(llmock.EchoResponder{})` for tests that need deterministic echo behavior
- Updated 11 tests across server_test.go, stream_test.go, admin_test.go, fault_test.go, and multiturn_test.go to use explicit `EchoResponder` where exact echo matching was needed
- Added `TestChatCompletions_DefaultRulesGreeting` to verify the new default behavior produces rule-based (not echo) responses
- All 155 tests pass, build clean, vet clean

## llmock-ete: MCP control plane: core tool definitions and dispatch

Added an MCP control plane endpoint (`POST /mcp/control`) that exposes llmock's admin functionality as MCP tools, enabling AI agents to control llmock's behavior via MCP:

- New `controlPlane` struct with references to `adminState` and `faultState` for accessing server internals
- `POST /mcp/control` endpoint using JSON-RPC 2.0 protocol, separate from the mock MCP server (`POST /mcp`)
- Implements `initialize`, `tools/list`, and `tools/call` MCP methods
- 9 control tools mirroring the existing `/_mock/*` admin HTTP API:
  - `llmock_add_rule` — add a response rule with regex pattern, responses array, and optional priority (0=prepend, -1=append, N=insert at index)
  - `llmock_list_rules` — return current rules as JSON
  - `llmock_reset_rules` — restore rules to initial startup configuration
  - `llmock_add_fault` — inject a fault (error, delay, timeout, malformed, rate_limit) with optional status, message, delay_ms, probability, count
  - `llmock_list_faults` — return active faults as JSON
  - `llmock_clear_faults` — remove all active faults
  - `llmock_list_requests` — return request log as JSON
  - `llmock_clear_requests` — clear the request log
  - `llmock_reset` — full reset: rules + faults + request log
- Each tool has a full JSON schema (`inputSchema`) for MCP client tool discovery
- Error responses use MCP's `isError: true` convention with descriptive error messages (invalid regex, missing required fields, etc.)
- Control plane is automatically enabled when admin API is enabled (default), disabled when admin API is disabled
- Coexists cleanly with the mock MCP server — `POST /mcp` simulates an MCP backend for testing MCP clients, while `POST /mcp/control` controls llmock itself
- New `control.go` with all tool definitions, dispatch logic, and tool call implementations
- 18 new tests: initialize handshake, tools/list with all 9 tools, add rule + verify via chat request, invalid regex error, rule priority, list rules, reset rules, add fault + verify via HTTP 503, list faults, clear faults, list requests, clear requests, full reset (rules + faults + log), unknown tool error, admin-disabled returns 404, method not found, invalid JSON parse error, coexistence with mock MCP server
- All 173 tests pass, build clean, vet clean, no race conditions

## llmock-ed6: MCP control plane: stdio transport for standalone use

Added stdio transport for the MCP control plane, enabling AI agents to interact with llmock's admin functionality via stdin/stdout without an HTTP server:

- New `StdioTransport` type in `stdio.go` that reads newline-delimited JSON-RPC 2.0 requests from an `io.Reader` and writes responses to an `io.Writer`
- `NewStdioTransport(s *Server)` constructor that creates a transport backed by the server's control plane; returns nil if admin API is disabled
- `Run(r io.Reader, w io.Writer)` reads requests line by line, dispatches through the control plane, and writes one JSON-RPC response per line
- Handles blank lines (skipped), invalid JSON (`-32700` parse error), invalid JSON-RPC version (`-32600`), and all method dispatch (initialize, tools/list, tools/call)
- Uses `bufio.Scanner` with 1 MB max line size for large JSON-RPC messages
- Write serialization via `sync.Mutex` for safe concurrent use
- All 9 control tools (add/list/reset rules, add/list/clear faults, list/clear requests, full reset) work identically over stdio as they do over HTTP
- CLI `--mcp-stdio` flag in `cmd/llmock/main.go`: when set, runs the control plane over stdin/stdout instead of starting the HTTP server; exits with error if admin API is disabled
- 13 new tests: initialize, tools/list, add rule + list rules, multiple requests in batch, invalid JSON, invalid JSON-RPC version, method not found, blank lines skipped, nil when admin disabled, request ID preservation (string and integer), full workflow (add rule + verify), add fault + clear fault, full reset
- All 186 tests pass, build clean, vet clean, no race conditions

## llmock-1al: Gemini/Vertex AI API backend

Added support for the Google Gemini API format so `genai.Client` can be pointed directly at llmock via `HTTPOptions.BaseURL`, eliminating the need for genai→OpenAI→genai adapters in consumers:

- New `gemini.go` file with complete Gemini API implementation:
  - `GeminiRequest` type: `contents` (role + parts), `systemInstruction`, `generationConfig` (temperature, maxOutputTokens, topP, topK), `tools` (functionDeclarations)
  - `GeminiContent`/`GeminiPart` types: uniform content model with `text`, `functionCall` (name + args), and `functionResponse` (name + response) parts
  - `GeminiResponse` type: `candidates` array with `content` (role + parts) and `finishReason`, plus `usageMetadata` (promptTokenCount, candidatesTokenCount, totalTokenCount)
- Two routes via prefix routing on `POST /v1beta/models/`:
  - `POST /v1beta/models/{model}:generateContent` — non-streaming responses
  - `POST /v1beta/models/{model}:streamGenerateContent?alt=sse` — SSE streaming responses
- Gemini-to-internal message conversion: maps `model` role to `assistant`, handles `systemInstruction` as system message, skips model messages with only function calls
- `functionResponse` parts extract `result` field text for rule matching in multi-turn tool use conversations
- Tool call responses: parts with `functionCall` (name + args) in the Gemini format
- Tool call validation: only returns tool calls matching function declarations in the request; falls through to text response otherwise
- Auto-tool-call generation works with Gemini `functionDeclarations` format, converting to internal `RequestTool` format
- SSE streaming: each `data:` line is a complete `GeminiResponse` JSON chunk; last chunk includes `finishReason: "STOP"` and `usageMetadata`
- Streaming tool calls: sent as a single SSE chunk with all function call parts
- Gemini-specific error format: `{"error": {"code": <int>, "message": "...", "status": "..."}}`
- Fault injection support: error faults, rate limit faults (429 + Retry-After), and timeout faults all use Gemini error format
- Model name extracted from URL path (e.g., `gemini-pro`, `gemini-1.5-flash`)
- Full integration with admin API: rule injection, request logging, fault injection all work with Gemini endpoint
- Cross-endpoint consistency: same messages produce identical responses on OpenAI, Anthropic, and Gemini endpoints
- 23 new tests: echo response with structure validation, usage metadata, empty contents error, invalid JSON error, model extraction from path, system instruction handling, default rules response, multi-turn conversation, SSE streaming with content reconstruction, non-stream still works, tool call response, tool call fall-through, auto-tool-calls, function response multi-turn, streaming tool call, error fault format, rate limit fault, admin rule injection, cross-endpoint consistency, request logging, stream empty contents error, stream invalid JSON error, config-loaded rules
- All 209 tests pass, build clean, vet clean, no race conditions
