## llmock-o9r: Core handler with hardcoded echo response (OpenAI format)

Implemented the foundation of the mock LLM API server:

- Created `Server` struct with functional options pattern (`Option`, `New()`, `Handler()`)
- Implemented `POST /v1/chat/completions` endpoint that echoes the last user message
- Response follows OpenAI ChatCompletion format: id, object, created, model, choices, usage
- Token estimation based on word count (~1.3 tokens/word + per-message overhead)
- Proper error handling: 400 for bad JSON/empty messages, 405 for wrong method, 404 for unknown paths
- `cmd/llmock/main.go` starts server on configurable port (flag, env var, default 9090)
- 8 tests using httptest covering: echo response, usage stats, error cases, default model, fallback behavior
- All tests pass, build clean, vet clean

## llmock-bw3: Anthropic Messages API format

Added support for the Anthropic `/v1/messages` endpoint alongside the existing OpenAI endpoint:

- Introduced `Responder` interface (`Respond([]InternalMessage) (string, error)`) as the common generation abstraction
- Both API handlers convert their format-specific messages to `InternalMessage` and delegate to the same `Responder`
- `EchoResponder` implements the interface (extracted from inline logic in the OpenAI handler)
- `POST /v1/messages` accepts Anthropic Messages API requests (model, messages, max_tokens)
- Returns valid Anthropic response: `id` with `msg_` prefix + random hex, `type:"message"`, `role:"assistant"`, content array with `type:"text"` blocks, `stop_reason:"end_turn"`, usage with `input_tokens`/`output_tokens`
- 5 new tests: echo response with full structure validation, usage stats, empty messages error, invalid JSON error, cross-endpoint content consistency
- All 13 tests pass, build clean, vet clean

## llmock-64l: Regex rule matching with template expansion

Added the core rule-matching engine that replaces echo responses with configurable regex-based pattern matching:

- `Rule` type: compiled `*regexp.Regexp` pattern + slice of response template strings
- `RuleResponder` implements `Responder` interface; evaluates rules in order, first match wins
- Template expansion: `$1`, `$2`, etc. for capture groups; `${input}` for full original message
- If no rule matches, falls back to a hardcoded default response
- `WithRules(rules...)` functional option to configure the server
- `WithResponder(r)` generic functional option for any `Responder` implementation
- `NewRuleResponder(rules)` constructor; uses built-in default rules when passed empty slice
- Built-in default rules: greetings, "I need X", "how do I X", "what is X", "help me X", "can/could/would you X", thanks, farewell, and a catchall — all styled as a helpful AI assistant
- `LoadRulesFile(path)` and `ParseRulesYAML(data)` for loading rules from YAML config files
- Added `gopkg.in/yaml.v3` dependency for YAML parsing
- 14 new tests: match priority, capture group substitution, ${input} expansion, no-match fallback, random selection among multiple templates, default rules applied, Anthropic endpoint integration, WithResponder option, YAML parsing (valid, invalid regex, empty responses), file loading (valid, missing file), dollar-sign edge case
- All 27 tests pass, build clean, vet clean

## llmock-rgg: SSE streaming responses

Added Server-Sent Events streaming support for both OpenAI and Anthropic API formats:

- When `stream: true` is set in the request, responses are streamed token-by-token via SSE
- OpenAI streaming format: `data: {...}\n\n` chunks with `chat.completion.chunk` object type, `delta` containing content, final chunk with `finish_reason: "stop"`, terminated by `data: [DONE]\n\n`
- Anthropic streaming format: full event sequence — `message_start` → `content_block_start` → multiple `content_block_delta` events → `content_block_stop` → `message_delta` (with `stop_reason` and `output_tokens`) → `message_stop`
- Tokenization splits response text into randomized 1-3 word chunks for natural-feeling streaming
- Configurable inter-token delay via `llmock.WithTokenDelay(d)` (default 15ms)
- Proper SSE headers: `Content-Type: text/event-stream`, `Cache-Control: no-cache`, `Connection: keep-alive`
- Uses `http.Flusher` to flush after each chunk for real streaming behavior
- Handles client disconnection gracefully via context cancellation
- Non-streaming requests continue to work unchanged for both API formats
- New `stream.go` file with all streaming logic; handlers in `server.go` branch on `req.Stream`
- 9 new tests: OpenAI chunk format validation, content reconstruction, non-stream still works, Anthropic event sequence validation, Anthropic content reconstruction, non-stream still works, message_delta stop_reason, WithTokenDelay, consistent ID across chunks
- All 36 tests pass, build clean, vet clean

## llmock-0we: Runtime rule injection API

Added HTTP endpoints under `/_mock/` to inject, inspect, and reset rules at runtime for integration testing:

- `POST /_mock/rules` — inject one or more rules with JSON body; rules are prepended by default (priority 0), appended with priority -1, or inserted at a specific index
- `GET /_mock/rules` — return current rule list as JSON (patterns as strings, responses as arrays)
- `DELETE /_mock/rules` — reset rules to the initial startup configuration
- `POST /_mock/reset` — full reset: rules back to startup state and request log cleared
- `GET /_mock/requests` — return log of last 100 requests with timestamps, matched rule, user message, and response
- `DELETE /_mock/requests` — clear the request log
- New `adminState` type with `sync.RWMutex` for thread-safe mutable rule list and request log
- `adminResponder` wraps admin state with a fallback to the original responder (e.g., EchoResponder) when no rule matches — injected rules work even on servers started without rules
- `WithAdminAPI(false)` option to disable the admin endpoints for production-like usage
- Admin API is enabled by default on all servers
- Refactored `extractInput()` helper and `errNoMessages` sentinel error as shared utilities across EchoResponder, RuleResponder, and adminResponder
- Request logging captures method, path, user message, matched rule pattern, and response text for each API request
- 16 new tests: rule injection with match verification, prepend/append priority, GET/DELETE rules, full reset, request log inspection and clearing, admin disabled returns 404, invalid regex/empty rules/empty responses validation, 100-entry log cap, default server admin endpoints, rule injection on echo-only server, concurrent access with race detector
- All 52 tests pass, build clean, vet clean, no race conditions

## llmock-6dk: Failure and delay injection

Added fault injection system for simulating failures, errors, and latency — critical for testing retry logic, timeouts, and error handling:

- Five fault types: `error` (HTTP error responses), `delay` (adds latency before normal response), `timeout` (accepts connection then hangs until client disconnects), `malformed` (returns invalid JSON or broken SSE), `rate_limit` (429 with Retry-After header)
- `Fault` struct with configurable fields: `Type`, `Status`, `Message`, `ErrorType`, `DelayMS`, `Probability` (0-1 for random firing), `Count` (auto-clearing after N triggers)
- Correct error response formats for both APIs: OpenAI `{"error":{"message":"...","type":"..."}}` and Anthropic `{"type":"error","error":{"type":"...","message":"..."}}`
- Faults evaluated before rules in the request pipeline — global faults fire first
- Go API: `llmock.WithFault(llmock.Fault{...})` option for startup configuration
- `llmock.WithSeed(int64)` option for deterministic fault probability in tests
- Admin API endpoints for runtime fault management:
  - `POST /_mock/faults` — inject faults with JSON body `{"faults":[...]}`
  - `GET /_mock/faults` — inspect current active faults
  - `DELETE /_mock/faults` — clear all faults
- Thread-safe `faultState` with mutex-protected fault list and dedicated `math/rand/v2.Rand` instance
- Count-based faults auto-remove from the active list when exhausted
- Timeout fault starts SSE stream (for streaming requests) then blocks, simulating mid-response timeout
- Malformed fault returns broken JSON for non-streaming and broken SSE for streaming requests
- 16 new tests: error faults on both OpenAI and Anthropic endpoints, rate limit with Retry-After header on both APIs, delay with timing verification, malformed for both stream and non-stream, timeout with context deadline, count-based auto-clearing, probability with fixed seed, admin API POST/GET/DELETE, fault-before-rules ordering, Go API WithFault, default error status
- All 68 tests pass, build clean, vet clean, no race conditions

## llmock-w2p: Markov chain text generator

Added a Markov chain text generator that produces LLM-ish filler text, used as a fallback and for padding template responses:

- `MarkovChain` type with configurable chain order (default 2 — bigram prefix), trained on whitespace-delimited tokens
- Generation stops at a configurable max token count or when it hits a natural sentence ending (`.`, `!`, `?`)
- Deterministic output with sorted key selection for reproducible generation given a fixed seed
- Default corpus embedded via `//go:embed` containing ~2000 words of "helpful AI assistant" style text — stereotypical LLM responses about architecture, testing, best practices, security, etc.
- `llmock.WithCorpus(r io.Reader)` option to provide a custom training corpus
- `llmock.WithCorpusFile(path string)` convenience option to load corpus from a file
- `llmock.WithSeed(int64)` now also seeds the Markov generator for deterministic test output
- `MarkovResponder` implements the `Responder` interface and serves as the fallback for `RuleResponder` when no rules match (replaces the old hardcoded fallback string)
- Template responses support `{{markov}}` (default 100 token limit) and `{{markov:N}}` (custom token limit) placeholders to splice Markov-generated text into rule responses
- `MarkovChain` is built once at startup and is safe for concurrent reads; `MarkovResponder` uses a mutex to protect its RNG
- `DefaultCorpusText()` exported function for test access to the embedded corpus
- `NewMarkovResponder(seed)` and `NewMarkovChain(order)` constructors for programmatic use
- 15 new tests: deterministic output with fixed seed, different seeds produce different output, output contains only corpus words (statistical), default corpus words-only validation, max token limit respected, empty corpus handling, order-1 chain, server integration (rule match vs Markov fallback), deterministic MarkovResponder, no-messages error, `{{markov}}` template expansion, `{{markov:N}}` with token limit, WithCorpus custom corpus, WithCorpusFile, Anthropic endpoint integration
- All 83 tests pass, build clean, vet clean, no race conditions

## llmock-5bz: YAML/JSON config file and CLI polish

Added config file support and made the standalone CLI server production-ready:

- `Config` struct unifies YAML/JSON file config and functional options into one source of truth
  - `ServerConfig`: port, admin_api toggle
  - `DefaultConfig`: token_delay_ms, seed, model
  - `RuleConfig`: pattern, responses, delay_ms
  - Supports `corpus_file` and `faults` array at the top level
- `LoadConfig(path)` reads a YAML or JSON config file (auto-detected by `.json` extension)
- `ParseConfig(data, path)` parses config from raw bytes
- `FindDefaultConfig()` auto-discovers `llmock.yaml` or `llmock.json` in the current directory
- `CompileRules(configs)` converts `RuleConfig` entries to compiled `Rule` values with validation
- `Config.ToOptions()` converts the full config into functional `Option` values for `New()`, so config files and programmatic options use the same code path; functional options applied after config options override config values
- CLI flags: `--config` (explicit config path), `--port` (overrides config), `--verbose` (log all requests to stderr)
- Port resolution priority: `--port` flag > config file > `PORT` env var > 9090
- Verbose mode middleware logs: method, path, HTTP status, and response time for each request
- `responseWriter` wrapper captures status codes and delegates `Flush()` for SSE compatibility
- Startup banner: logs config source, port, number of rules loaded, corpus source, admin API status
- Graceful shutdown on SIGINT/SIGTERM with 5-second timeout via `http.Server.Shutdown()`
- 15 new tests: YAML parsing (full config), JSON parsing, invalid YAML/JSON errors, file loading (valid and missing), `FindDefaultConfig` (none, yaml, json), `CompileRules` (valid, invalid regex, no responses), `ToOptions` (full config, invalid rule, empty config), config with faults, token delay option
- All 98 tests pass, build clean, vet clean, no race conditions
