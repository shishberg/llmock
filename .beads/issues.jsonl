{"id":"llmock-0lw","title":"mockllm: MCP server simulation","description":"Add a simulated MCP (Model Context Protocol) server that can be used to test\nMCP client integrations.\n\n## What this does\nThe server can act as an MCP server, advertising tools and resources and\nresponding to MCP protocol messages — enabling testing of MCP client code\nwithout a real MCP server.\n\n## Requirements\n- Implement the MCP protocol over HTTP+SSE transport (streamable HTTP):\n  - `POST /mcp` — main MCP endpoint accepting JSON-RPC 2.0 messages\n  - Support the core MCP methods:\n    - `initialize` — return server capabilities and info\n    - `tools/list` — return configured tools\n    - `tools/call` — execute a tool call and return results\n    - `resources/list` — return configured resources\n    - `resources/read` — return resource content\n    - `prompts/list` — return configured prompts\n    - `prompts/get` — return prompt content\n- MCP tools, resources, and prompts are configurable:\n  ```yaml\n  mcp:\n    tools:\n      - name: \"get_weather\"\n        description: \"Get current weather for a location\"\n        input_schema:\n          type: object\n          properties:\n            location: { type: string }\n          required: [location]\n        responses:\n          - pattern: \".*\"\n            result: '{\"temperature\": 72, \"condition\": \"sunny\"}'\n    resources:\n      - uri: \"file:///project/README.md\"\n        name: \"Project README\"\n        content: \"# My Project\\nThis is a mock project.\"\n    prompts:\n      - name: \"review_code\"\n        description: \"Review code for issues\"\n        arguments:\n          - name: \"language\"\n            required: true\n        template: \"Please review the following {{language}} code...\"\n  ```\n- MCP tool call responses support the same rule-matching as the main API:\n  pattern matching on the tool arguments, template expansion, Markov filler\n- Runtime injection via `/_mock/mcp/tools`, `/_mock/mcp/resources`, etc.\n- Tests: full MCP handshake, tool listing, tool calling with pattern matching,\n  resource reading, prompt retrieval\n\n## Design notes\n- MCP uses JSON-RPC 2.0 — make sure to handle request IDs correctly\n- The streamable HTTP transport uses SSE for server-to-client messages\n- MCP is optional — disabled by default, enabled via config or\n  `mockllm.WithMCP(true)`\n- This can share the rule-matching and Markov infrastructure with the main\n  chat API handlers\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T10:24:33.03007+11:00","updated_at":"2026-02-07T10:24:33.11264+11:00","dependencies":[{"issue_id":"llmock-0lw","depends_on_id":"llmock-lp1","type":"blocks","created_at":"2026-02-07T10:24:33.209894+11:00","created_by":"David McLeish"}]}
{"id":"llmock-0we","title":"mockllm: Runtime rule injection API","description":"Add HTTP endpoints to inject, inspect, and reset rules at runtime — the\nescape hatch for integration tests.\n\n## What this does\nTests can configure the mock server's behavior on the fly without restarting,\nusing a simple HTTP API under the `/_mock/` prefix.\n\n## Requirements\n- `POST /_mock/rules` — add one or more rules. Request body:\n  ```json\n  {\n    \"rules\": [\n      {\n        \"pattern\": \".*deploy.*\",\n        \"responses\": [\"Deploying now...\"],\n        \"priority\": 0\n      }\n    ]\n  }\n  ```\n  Rules added via API are prepended (higher priority) to the rule list by default.\n  Optional `priority` field: 0 = prepend (default), -1 = append, or an integer\n  index to insert at.\n- `GET /_mock/rules` — return the current rule list as JSON (patterns as strings,\n  not compiled regexps obviously)\n- `DELETE /_mock/rules` — reset to the initial rules from config/startup\n- `POST /_mock/reset` — full reset: rules, request log, everything back to\n  startup state\n- `GET /_mock/requests` — return a log of recent requests (last 100) with\n  timestamps, matched rule (if any), and response summary. Invaluable for\n  debugging integration tests.\n- `DELETE /_mock/requests` — clear the request log\n- Thread safety: all of this must be safe under concurrent access. The rule list\n  is now mutable, so use appropriate synchronization (RWMutex on the rule list).\n- Tests: inject a rule, send a matching request, verify it matches; reset,\n  verify it no longer matches; inspect the request log\n\n## Design notes\n- The /_mock/ endpoints should be optionally disableable for production-like\n  usage via `mockllm.WithAdminAPI(false)`\n- Consider a Go helper for tests:\n  ```go\n  mock := mockllm.NewTestHelper(ts.URL)\n  mock.AddRule(\".*error.*\", \"Something went wrong\")\n  defer mock.Reset()\n  ```\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T10:24:31.788717+11:00","updated_at":"2026-02-07T10:24:31.876323+11:00","dependencies":[{"issue_id":"llmock-0we","depends_on_id":"llmock-64l","type":"blocks","created_at":"2026-02-07T10:24:31.97373+11:00","created_by":"David McLeish"}]}
{"id":"llmock-5bz","title":"mockllm: YAML/JSON config file and CLI polish","description":"Add config file support and make the standalone CLI server production-ready.\n\n## What this does\nThe standalone `mockllm` binary can be fully configured from a YAML or JSON\nconfig file, with sensible defaults and good DX.\n\n## Requirements\n- Config file format (YAML and JSON both supported):\n  ```yaml\n  server:\n    port: 9090\n    admin_api: true\n\n  defaults:\n    token_delay_ms: 15\n    seed: 0\n    model: \"mock-llm-1\"\n\n  rules:\n    - pattern: \".*hello.*\"\n      responses: [\"Hi there! How can I help you today?\"]\n    - pattern: \"how do I (.*)\"\n      responses:\n        - \"Here is how you can $1: {{markov:50}}\"\n      delay_ms: 200\n\n  corpus_file: \"./my-corpus.txt\"\n\n  faults: []\n  ```\n- CLI flags: `--config`, `--port` (overrides config), `--verbose` (log all\n  requests/responses to stderr)\n- If no config file specified, look for `mockllm.yaml` or `mockllm.json` in\n  the current directory, otherwise use defaults\n- Verbose mode logs: timestamp, method, path, matched rule (or \"fallback\"),\n  response status, response time\n- Startup banner showing: port, number of rules loaded, corpus size, admin API\n  status\n- Graceful shutdown on SIGINT/SIGTERM\n- README.md with:\n  - Quick start (go install + run)\n  - Library usage with httptest\n  - Config file reference\n  - Examples for common test scenarios\n- Tests for config loading, CLI flag parsing, default config behavior\n\n## Design notes\n- Use a single Config struct that both the YAML loader and functional options\n  populate, so there is one source of truth\n- The functional options (WithRules, WithCorpus, etc) should override config\n  file values when both are provided\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T10:24:32.418727+11:00","updated_at":"2026-02-07T10:24:32.504691+11:00","dependencies":[{"issue_id":"llmock-5bz","depends_on_id":"llmock-6dk","type":"blocks","created_at":"2026-02-07T10:24:32.59414+11:00","created_by":"David McLeish"},{"issue_id":"llmock-5bz","depends_on_id":"llmock-w2p","type":"blocks","created_at":"2026-02-07T10:24:32.685603+11:00","created_by":"David McLeish"}]}
{"id":"llmock-64l","title":"mockllm: Regex rule matching with template expansion","description":"Add the core rule-matching engine: configurable regex rules that match against\nuser messages and produce templated responses.\n\n## What this does\nInstead of echoing, the server now matches user messages against an ordered list\nof regex rules and expands response templates with capture groups.\n\n## Requirements\n- A `Rule` type: compiled regex pattern + response template string (or list of\n  strings to pick from randomly)\n- Rules are evaluated in order; first match wins\n- Template expansion supports:\n  - `$1`, `$2`, etc for regex capture groups\n  - `${input}` for the full original user message\n- If no rule matches, fall back to a default response (hardcoded for now,\n  will become Markov later)\n- Server accepts rules via functional options: `mockllm.WithRules(rules...)`\n- Ship a small set of built-in default rules that produce ELIZA-like responses:\n  - \"I need (.*)\" -\u003e \"Why do you need $1?\" / \"What would it mean if you got $1?\"\n  - \"how do I (.*)\" -\u003e \"Here's how you can approach $1: first, ...\"\n  - \"what is (.*)\" -\u003e \"That's a great question. $1 refers to ...\"\n  - \"help me (.*)\" -\u003e \"I'd be happy to help you $1. Let me break this down...\"\n  - General greetings, farewells, etc.\n  Make these feel more like a helpful AI assistant than a psychotherapist.\n- Rules should be loadable from a YAML config file:\n  ```yaml\n  rules:\n    - pattern: \"deploy (.*) to (.*)\"\n      responses:\n        - \"To deploy $1 to $2, you will want to follow these steps...\"\n        - \"Deploying $1 to $2 requires careful planning. Here is what I recommend...\"\n    - pattern: \".*\"\n      responses:\n        - \"That is an interesting point. Could you tell me more?\"\n  ```\n- Tests covering: match priority, capture group substitution, no-match fallback,\n  random selection among multiple response templates\n\n## Design notes\n- Rules should be safe for concurrent access (they are read-only after init,\n  but runtime injection is coming later)\n- Use `regexp.MustCompile` at config time, not per-request\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T10:24:31.003651+11:00","updated_at":"2026-02-07T10:24:31.094465+11:00","dependencies":[{"issue_id":"llmock-64l","depends_on_id":"llmock-bw3","type":"blocks","created_at":"2026-02-07T10:24:31.182187+11:00","created_by":"David McLeish"}]}
{"id":"llmock-6dk","title":"mockllm: Failure and delay injection","description":"Add the ability to simulate failures, errors, and latency — critical for\ntesting retry logic, timeouts, and error handling.\n\n## What this does\nThe server can be configured to return errors, inject delays, or behave\nbadly in controllable ways.\n\n## Requirements\n- Per-rule failure injection in the rule config:\n  ```yaml\n  rules:\n    - pattern: \".*deploy.*prod.*\"\n      error:\n        status: 529\n        type: \"overloaded_error\"\n        message: \"Overloaded\"\n      probability: 0.5  # 50% chance of error vs normal response\n  ```\n- Global failure modes settable via API:\n  - `POST /_mock/faults` with body:\n    ```json\n    {\n      \"type\": \"error\",\n      \"status\": 500,\n      \"message\": \"Internal server error\",\n      \"delay_ms\": 5000,\n      \"probability\": 1.0,\n      \"count\": 3\n    }\n    ```\n  - `DELETE /_mock/faults` — clear all faults\n- Fault types:\n  - `error`: return the specified HTTP status + error body in the correct API format\n    (OpenAI and Anthropic have different error response schemas)\n  - `delay`: add latency before responding (works with both streaming and non-streaming)\n  - `timeout`: accept the connection, start streaming (if applicable), then hang\n    and never finish — simulates a mid-response timeout\n  - `malformed`: return invalid JSON / broken SSE stream — for testing parser resilience\n  - `rate_limit`: return 429 with Retry-After header and appropriate rate limit\n    error bodies for each API format\n- Per-rule delays: any rule can have a `delay_ms` field\n- Faults are evaluated before rules: if a global fault matches (by probability),\n  it fires instead of the normal pipeline\n- The Go API should support this too:\n  ```go\n  srv := mockllm.New(\n      mockllm.WithFault(mockllm.Fault{\n          Type: mockllm.FaultRateLimit,\n          Count: 2,  // first 2 requests get 429, then normal\n      }),\n  )\n  ```\n- Tests: verify each fault type produces the correct output, test probability-based\n  faults with a fixed seed, test count-based auto-clearing\n\n## Design notes\n- Error response formats differ between OpenAI and Anthropic — make sure both\n  are correct:\n  - OpenAI: `{\"error\":{\"message\":\"...\",\"type\":\"...\",\"code\":\"...\"}}`\n  - Anthropic: `{\"type\":\"error\",\"error\":{\"type\":\"...\",\"message\":\"...\"}}`\n- The `timeout` fault is the trickiest — you need to hold the connection open.\n  Use a context with a very long timer or wait for client disconnect.\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T10:24:32.059954+11:00","updated_at":"2026-02-07T10:24:32.146454+11:00","dependencies":[{"issue_id":"llmock-6dk","depends_on_id":"llmock-0we","type":"blocks","created_at":"2026-02-07T10:24:32.238417+11:00","created_by":"David McLeish"},{"issue_id":"llmock-6dk","depends_on_id":"llmock-rgg","type":"blocks","created_at":"2026-02-07T10:24:32.335972+11:00","created_by":"David McLeish"}]}
{"id":"llmock-abr","title":"x","status":"tombstone","priority":2,"issue_type":"task","owner":"dave@mcleish.id.au","created_at":"2026-02-07T10:15:43.803115+11:00","created_by":"David McLeish","updated_at":"2026-02-07T10:24:16.3022+11:00","deleted_at":"2026-02-07T10:24:16.3022+11:00","deleted_by":"David McLeish","delete_reason":"delete","original_type":"task"}
{"id":"llmock-bw3","title":"mockllm: Anthropic Messages API format","description":"Add support for the Anthropic `/v1/messages` endpoint alongside the existing\nOpenAI endpoint.\n\n## What this does\nThe server now speaks both OpenAI and Anthropic API formats.\n\n## Requirements\n- `POST /v1/messages` accepts an Anthropic Messages API request\n  (model, messages array, max_tokens, optional stream bool)\n- Return a valid Anthropic Messages response (id, type:\"message\", role:\"assistant\",\n  content array with type:\"text\" blocks, model, stop_reason:\"end_turn\",\n  usage with input_tokens/output_tokens)\n- The actual response content uses the same internal generation logic as the OpenAI\n  endpoint (currently just echo) — both endpoints should call into the same\n  response generator interface\n- Define a clean internal interface/type for \"generate a response given a conversation\"\n  that both API formats call into. Something like:\n  `type Responder interface { Respond(messages []Message) (string, error) }`\n  where Message is an internal type that both API formats convert to/from\n- Tests for the Anthropic endpoint validating response structure\n- Tests confirming both endpoints produce the same logical content for the same input\n\n## Design notes\n- The Anthropic format uses `content: [{\"type\":\"text\",\"text\":\"...\"}]` not a plain string\n- `stop_reason` is `end_turn` not `stop`\n- Message id format should look like `msg_` prefix + random hex\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T10:24:30.732526+11:00","updated_at":"2026-02-07T10:24:30.825818+11:00","dependencies":[{"issue_id":"llmock-bw3","depends_on_id":"llmock-o9r","type":"blocks","created_at":"2026-02-07T10:24:30.919687+11:00","created_by":"David McLeish"}]}
{"id":"llmock-lp1","title":"mockllm: Tool use / function calling simulation","description":"Add support for simulating tool/function calls in responses — when the request\nincludes tool definitions, the mock can respond with tool calls.\n\n## What this does\nThe server can inspect tool definitions provided in the request and generate\nresponses that include tool calls, enabling testing of tool-use workflows.\n\n## Requirements\n- Parse tool/function definitions from requests:\n  - OpenAI format: `tools` array with `type:\"function\"` and function name/description/parameters\n  - Anthropic format: `tools` array with name/description/input_schema\n- New rule fields for tool call responses:\n  ```yaml\n  rules:\n    - pattern: \".*weather.*\"\n      tool_call:\n        name: \"get_weather\"\n        arguments:\n          location: \"$1\"\n          unit: \"celsius\"\n  ```\n  If the named tool is not in the request, fall through to next rule.\n- If a rule specifies a tool_call, respond in the correct format:\n  - OpenAI: `choices[0].message.tool_calls` array with id, type, function name/arguments\n  - Anthropic: `content` array with `type:\"tool_use\"` block with id, name, input\n- Support multi-turn tool use: if the request includes a tool_result message\n  (user providing tool output), subsequent rules can match against the tool\n  output content\n- Auto-generation mode: if `auto_tools: true` is set and no rule matches but\n  tools are defined in the request, pick a random tool and generate plausible\n  arguments based on the JSON schema (strings get Markov text, numbers get\n  random values in range, booleans get random true/false, enums pick a random value)\n- Streaming tool calls: both OpenAI and Anthropic have specific streaming formats\n  for tool calls — implement these\n- Tests: tool call response format validation for both APIs, multi-turn tool use\n  conversation, auto-generation with schema-based arguments, streaming tool calls\n\n## Design notes\n- Tool call IDs should look realistic: OpenAI uses `call_` + alphanumeric,\n  Anthropic uses `toolu_` + alphanumeric\n- The auto-generation of arguments from JSON schema does not need to be perfect —\n  this is for testing that your code handles the shape correctly, not the content\n- For multi-turn, you need to handle the message history, not just the last message\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T10:24:32.768858+11:00","updated_at":"2026-02-07T10:24:32.858759+11:00","dependencies":[{"issue_id":"llmock-lp1","depends_on_id":"llmock-5bz","type":"blocks","created_at":"2026-02-07T10:24:32.94735+11:00","created_by":"David McLeish"}]}
{"id":"llmock-o9r","title":"mockllm: Core handler with hardcoded echo response (OpenAI format)","description":"Build the foundation of a mock LLM API server in Go.\n\n## What this does\nCreate a Go module `mockllm` that exposes an `http.Handler` implementing the\nOpenAI `/v1/chat/completions` endpoint. For now it returns a hardcoded/echo\nresponse — the intelligence comes later.\n\n## Requirements\n- Go module at `github.com/OWNER/mockllm` (use a placeholder module path for now)\n- A `Server` struct with a `Handler() http.Handler` method\n- `POST /v1/chat/completions` accepts an OpenAI ChatCompletion request\n  (model, messages array with role/content, optional stream bool, temperature, max_tokens)\n- For now: respond with the last user message echoed back, wrapped in a valid\n  OpenAI ChatCompletion response JSON (id, object, created, model, choices, usage)\n- Generate plausible-looking usage stats (prompt_tokens, completion_tokens, total_tokens)\n  based on rough word counts\n- Ignore the `stream` field for now (always return non-streaming)\n- Wire up a `cmd/mockllm/main.go` that starts the server on a configurable port\n  (flag or env var, default 9090)\n- Include tests that use `httptest.NewServer` with the handler, send a request\n  via raw HTTP, and validate the response structure\n\n## Design notes\n- Keep the Server struct ready to accept configuration (rules, corpus, etc) even\n  though we are not using them yet — use an Options pattern or functional options\n- The handler should use a mux (stdlib `http.ServeMux` is fine) so we can add\n  more routes later\n- Return proper HTTP error codes for malformed requests (400), wrong method (405),\n  unknown paths (404)\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T10:24:30.559764+11:00","updated_at":"2026-02-07T10:24:30.647997+11:00"}
{"id":"llmock-rgg","title":"mockllm: SSE streaming responses","description":"Add Server-Sent Events streaming support for both OpenAI and Anthropic formats.\n\n## What this does\nWhen `stream: true` is set in the request, the server streams the response\ntoken-by-token using the appropriate SSE format for each API.\n\n## Requirements\n- OpenAI streaming format:\n  - Content-Type: text/event-stream\n  - Each chunk: `data: {\"id\":\"...\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"word \"},\"finish_reason\":null}]}\\n\\n`\n  - Final chunk has `finish_reason: \"stop\"` and empty delta\n  - Ends with `data: [DONE]\\n\\n`\n- Anthropic streaming format:\n  - Content-Type: text/event-stream\n  - Event sequence: `message_start` -\u003e `content_block_start` -\u003e multiple\n    `content_block_delta` events -\u003e `content_block_stop` -\u003e `message_delta` -\u003e `message_stop`\n  - Each content_block_delta: `event: content_block_delta\\ndata: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"word \"}}\\n\\n`\n- Tokenization: split the generated response into chunks of 1-3 words\n  (randomized to feel natural)\n- Configurable inter-token delay (default 15ms) via `mockllm.WithTokenDelay(d)`\n  to simulate generation speed\n- Flush after each chunk (ensure streaming actually streams, not buffered)\n- Tests using a streaming HTTP client that reads chunks incrementally and\n  validates the event format\n- Both streaming and non-streaming should work for both API formats\n\n## Design notes\n- Set appropriate headers: Cache-Control: no-cache, Connection: keep-alive\n- Use `http.Flusher` interface to flush after each write\n- Handle client disconnection gracefully (context cancellation)\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T10:24:31.529535+11:00","updated_at":"2026-02-07T10:24:31.619673+11:00","dependencies":[{"issue_id":"llmock-rgg","depends_on_id":"llmock-bw3","type":"blocks","created_at":"2026-02-07T10:24:31.706021+11:00","created_by":"David McLeish"}]}
{"id":"llmock-w2p","title":"mockllm: Markov chain text generator","description":"Add a Markov chain text generator that produces LLM-ish filler text, used as\na fallback and for padding template responses.\n\n## What this does\nWhen no rule matches, or when a template includes a `{{markov}}` placeholder,\nthe server generates plausible-sounding \"helpful assistant\" text using a\nMarkov chain.\n\n## Requirements\n- A `MarkovChain` type that can be trained on a text corpus and generate text\n- Configurable chain order (default 2 — bigram prefix)\n- Generation stops at a configurable max token count or when it hits a natural\n  sentence ending\n- Ship a default corpus embedded via `//go:embed` containing ~2000 words of\n  \"helpful AI assistant\" style text. Write this corpus yourself — paragraphs like:\n  \"That is a great question. Let me break this down step by step. First, you will\n  want to consider the overall architecture of your system. There are several\n  approaches you could take, each with different tradeoffs...\"\n  The goal is that Markov-generated output from this corpus reads like a\n  stereotypical LLM response at a glance.\n- `mockllm.WithCorpus(r io.Reader)` option to provide a custom training corpus\n- `mockllm.WithCorpusFile(path string)` convenience option\n- The Markov generator becomes the default fallback Responder when no rules match\n- Template responses can include `{{markov}}` or `{{markov:50}}` (with token limit)\n  to splice in generated text\n- Tests: deterministic output with a fixed seed, statistical tests that output\n  only contains words from the corpus, integration test showing it plugs into\n  the response pipeline\n\n## Design notes\n- The chain should be built once at startup and be safe for concurrent reads\n- Use a deterministic seed option for testing: `mockllm.WithSeed(int64)`\n- Token splitting can just be whitespace-based, nothing fancy\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T10:24:31.26535+11:00","updated_at":"2026-02-07T10:24:31.353966+11:00","dependencies":[{"issue_id":"llmock-w2p","depends_on_id":"llmock-64l","type":"blocks","created_at":"2026-02-07T10:24:31.446913+11:00","created_by":"David McLeish"}]}
